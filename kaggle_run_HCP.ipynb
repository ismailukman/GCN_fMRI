{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGCN fMRI Analysis on Kaggle\n",
    "## Running from GitHub with GPU/TPU Support\n",
    "\n",
    "This notebook loads the cGCN implementation directly from GitHub and runs it on Kaggle with GPU/TPU optimization.\n",
    "\n",
    "**Note:** Make sure to enable GPU in Kaggle Notebook Settings: Settings → Accelerator → GPU T4 x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository\n",
    "!git clone https://github.com/ismailukman/GCN_fMRI.git\n",
    "%cd GCN_fMRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU/TPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Check for GPU\n",
    "print(f\"\\nGPU Available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# List all available devices\n",
    "print(\"\\nAvailable devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Enable memory growth for GPU to avoid OOM errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\nMemory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Data Files\n",
    "\n",
    "### Option A: Upload data files to Kaggle Datasets\n",
    "1. Download the data files from the links in README.md\n",
    "2. Upload them as a Kaggle dataset\n",
    "3. Add the dataset to this notebook\n",
    "4. Uncomment and modify the paths below\n",
    "\n",
    "### Option B: Direct download from Google Drive (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gdown for Google Drive downloads\n",
    "!pip install -q gdown\n",
    "\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('.', exist_ok=True)\n",
    "\n",
    "print(\"Downloading HCP dataset...\")\n",
    "# HCP.h5 (864 MB)\n",
    "gdown.download('https://drive.google.com/uc?id=1l029ZuOIUY5gehBZCAyHaJqMNuxRHTFc', 'HCP.h5', quiet=False)\n",
    "\n",
    "print(\"\\nDownloading FC matrix...\")\n",
    "# FC.npy\n",
    "gdown.download('https://drive.google.com/uc?id=1WP4_9bps-NbX6GNBnhFu8itV3y1jriJL', 'FC.npy', quiet=False)\n",
    "\n",
    "print(\"\\nData files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Check if files exist and their sizes\n",
    "files_to_check = ['HCP.h5', 'FC.npy']\n",
    "for file in files_to_check:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"✓ {file}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"✗ {file}: NOT FOUND\")\n",
    "\n",
    "# Load and inspect HCP data\n",
    "with h5py.File('HCP.h5', 'r') as f:\n",
    "    print(\"\\nHCP.h5 contents:\")\n",
    "    for key in f.keys():\n",
    "        print(f\"  {key}: {f[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure GPU/TPU Settings for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mixed precision for better GPU performance\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision training (float16 with float32 accumulators)\n",
    "# This can significantly speed up training on modern GPUs\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"Variable dtype: {policy.variable_dtype}\")\n",
    "\n",
    "# Configure TensorFlow for optimal GPU usage\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA compilation\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "print(\"\\nGPU optimization configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Import Model and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from the cloned repository\n",
    "from model import get_model\n",
    "from utils import save_logs_models\n",
    "\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from time import gmtime, strftime\n",
    "\n",
    "print(\"Model imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_N = 236\n",
    "frames = 100\n",
    "\n",
    "# Load HCP data\n",
    "with h5py.File('HCP.h5', 'r') as f:\n",
    "    x_train, x_val, x_test = f['x_train'][()], f['x_val'][()], f['x_test'][()]\n",
    "    y_train, y_val, y_test = f['y_train'][()], f['y_val'][()], f['y_test'][()]\n",
    "\n",
    "# Add channel dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_val = np.expand_dims(x_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "\n",
    "# Convert to categorical\n",
    "num_classes = 100\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\ny_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "k = 5  # Number of nearest neighbors\n",
    "batch_size = 16  # Increased for GPU (original: 8)\n",
    "epochs = 100\n",
    "l2_reg = 1e-4\n",
    "dp = 0.5\n",
    "lr = 1e-5\n",
    "\n",
    "print(f\"Dropout: {dp}\")\n",
    "print(f\"L2 regularization: {l2_reg}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"k (neighbors): {k}\")\n",
    "\n",
    "# Setup output directory\n",
    "file_name = f'kaggle_k_{k}_l2_{l2_reg}_dp_{dp}'\n",
    "print(f\"\\nFile name: {file_name}\")\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "tmp_name = f'tmp/tmp_{file_name}_{strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())}.hdf5'\n",
    "print(f\"Model checkpoint: {tmp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = get_model(\n",
    "    graph_path='FC.npy',\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    dp=dp,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=None,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "lr_hist = []\n",
    "\n",
    "class Lr_record(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        tmp = K.get_value(model.optimizer.learning_rate)\n",
    "        lr_hist.append(tmp)\n",
    "        print(f'Learning rate: {tmp}')\n",
    "\n",
    "lr_record = Lr_record()\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    filepath=tmp_name,\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# TensorBoard callback for visualization\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Monitor training progress in real-time!\\n\")\n",
    "\n",
    "# Train model\n",
    "model_history = model.fit(\n",
    "    x_train, y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[checkpointer, lr_record, reduce_lr, earlystop, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading best model for evaluation...\")\n",
    "\n",
    "# Load best model\n",
    "model_best = get_model(\n",
    "    graph_path='FC.npy',\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=tmp_name,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model_best.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_results = model_best.evaluate(x=x_val, y=y_val, batch_size=batch_size, verbose=1)\n",
    "print(f\"\\nValidation - Loss: {val_results[0]:.4f}, Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = model_best.evaluate(x=x_test, y=y_test, batch_size=batch_size, verbose=1)\n",
    "print(f\"Test - Loss: {test_results[0]:.4f}, Accuracy: {test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save logs and models\n",
    "save_logs_models(\n",
    "    model,\n",
    "    model_history,\n",
    "    acc=val_results[1],\n",
    "    folder='tmp/',\n",
    "    lr_hist=lr_hist,\n",
    "    file_name=file_name,\n",
    "    loss_name='loss',\n",
    "    acc_name='accuracy',\n",
    "    tmp_name=tmp_name\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(model_history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(model_history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(model_history.history['loss'], label='Train Loss')\n",
    "ax2.plot(model_history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Best Validation Accuracy: {max(model_history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Test Accuracy: {test_results[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
