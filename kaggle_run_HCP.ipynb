{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# cGCN fMRI Analysis on Kaggle\n## Running from GitHub with GPU/TPU Support\n\nThis notebook loads the cGCN implementation directly from GitHub and runs it on Kaggle with GPU/TPU optimization.\n\n## ⚙️ Pre-configured Settings\n\nThis notebook is pre-configured with:\n- ✅ **GPU T4 Accelerator** - Automatically enabled for faster training\n- ✅ **Internet Access** - Required to clone code from GitHub\n\n## ⚠️ IMPORTANT: Before Running This Notebook\n\n**You MUST enable Internet if not already ON:**\n- Go to Settings (right panel) → Internet → Turn ON\n\n**Add your dataset:**\n- Click \"Add Data\" → Search for your fMRI dataset and add it\n- The notebook expects data at: `/kaggle/input/fmri-data/`\n- Should contain: \n  - `HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5`\n  - `FC.npy`\n\n---\n\n**Troubleshooting:**\n- If you see \"Could not resolve host: github.com\" → Internet is OFF\n- If GPU is not detected → Check Settings → Accelerator → Should be \"GPU T4\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport subprocess\n\n# Check if repository already exists\nif os.path.exists('GCN_fMRI'):\n    print(\"✓ Repository already exists, using existing clone\")\n    os.chdir('GCN_fMRI')\nelse:\n    print(\"Cloning GitHub repository...\")\n    print(\"Repository: https://github.com/ismailukman/GCN_fMRI.git\\n\")\n    \n    # Use subprocess to capture output\n    try:\n        result = subprocess.run(\n            ['git', 'clone', 'https://github.com/ismailukman/GCN_fMRI.git'],\n            capture_output=True,\n            text=True,\n            timeout=60\n        )\n        \n        # Show git output\n        if result.stdout:\n            print(result.stdout)\n        if result.stderr:\n            print(result.stderr)\n        \n        # Check if clone was successful\n        if not os.path.exists('GCN_fMRI'):\n            print(\"\\n\" + \"=\"*70)\n            print(\"ERROR: Failed to clone repository from GitHub\")\n            print(\"=\"*70)\n            print(\"\\nGit command output above shows the error.\")\n            print(\"\\nCommon solutions:\")\n            print(\"1. Check that Internet is enabled: Settings → Internet → ON\")\n            print(\"2. If Internet is ON, try running this cell again\")\n            print(\"3. Check repository exists: https://github.com/ismailukman/GCN_fMRI\")\n            print(\"\\nIf the error persists, you can manually download and upload the files:\")\n            print(\"- Go to: https://github.com/ismailukman/GCN_fMRI\")\n            print(\"- Download ZIP and extract\")\n            print(\"- Upload model.py, utils.py, and other Python files to this notebook\")\n            print(\"=\"*70)\n            raise RuntimeError(\"Cannot proceed without repository files.\")\n        \n        os.chdir('GCN_fMRI')\n        print(\"\\n✓ Repository cloned successfully\")\n        \n    except subprocess.TimeoutExpired:\n        print(\"\\n✗ Git clone timed out after 60 seconds\")\n        print(\"Network might be slow. Try running this cell again.\")\n        raise\n    except FileNotFoundError:\n        print(\"\\n✗ Git command not found\")\n        print(\"This shouldn't happen on Kaggle. Please report this issue.\")\n        raise\n\n# Add the repository to Python path so imports work\nrepo_path = os.getcwd()\nif repo_path not in sys.path:\n    sys.path.insert(0, repo_path)\n    print(f\"✓ Added {repo_path} to Python path\")\n\nprint(f\"\\nCurrent directory: {repo_path}\")\nprint(f\"Files in directory: {', '.join(os.listdir(repo_path)[:10])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU/TPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Check for GPU\n",
    "print(f\"\\nGPU Available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# List all available devices\n",
    "print(\"\\nAvailable devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Enable memory growth for GPU to avoid OOM errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\nMemory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Kaggle Dataset\n",
    "\n",
    "This notebook uses fMRI data from a Kaggle dataset.\n",
    "Make sure you have added the dataset to this notebook:\n",
    "- Click \"Add Data\" in the right panel\n",
    "- Search for and add your fMRI dataset\n",
    "- The data should be available at `/kaggle/input/fmri-data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport h5py\n\n# Define Kaggle dataset path\nKAGGLE_DATA_PATH = '/kaggle/input/fmri-data/'\n\n# Check if dataset directory exists\nif os.path.exists(KAGGLE_DATA_PATH):\n    print(f\"✓ Kaggle dataset found at: {KAGGLE_DATA_PATH}\")\n    print(\"\\nDataset contents:\")\n    for item in os.listdir(KAGGLE_DATA_PATH):\n        item_path = os.path.join(KAGGLE_DATA_PATH, item)\n        if os.path.isfile(item_path):\n            size_mb = os.path.getsize(item_path) / (1024 * 1024)\n            print(f\"  {item}: {size_mb:.2f} MB\")\nelse:\n    print(f\"✗ Dataset not found at: {KAGGLE_DATA_PATH}\")\n    print(\"\\nPlease add the fMRI dataset to this notebook:\")\n    print(\"1. Click 'Add Data' in the right panel\")\n    print(\"2. Search for your fMRI dataset\")\n    print(\"3. Click 'Add' to attach it to this notebook\")\n    raise FileNotFoundError(f\"Dataset not found at {KAGGLE_DATA_PATH}\")\n\n# Define file paths\nHCP_PATH = os.path.join(KAGGLE_DATA_PATH, 'HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5')\nFC_PATH = os.path.join(KAGGLE_DATA_PATH, 'FC.npy')\n\n# Verify required files\nprint(\"\\nVerifying required files:\")\nfiles_to_check = {'HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5': HCP_PATH, 'FC.npy': FC_PATH}\nfor name, path in files_to_check.items():\n    if os.path.exists(path):\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        print(f\"  ✓ {name}: {size_mb:.2f} MB\")\n    else:\n        print(f\"  ✗ {name}: NOT FOUND\")\n        raise FileNotFoundError(f\"Required file not found: {path}\")\n\n# Inspect HCP data structure\nprint(\"\\nHCP data structure:\")\nwith h5py.File(HCP_PATH, 'r') as f:\n    for key in f.keys():\n        print(f\"  {key}: {f[key].shape}\")\n\nprint(\"\\n✓ All required files verified!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure GPU/TPU Settings for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mixed precision for better GPU performance\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision training (float16 with float32 accumulators)\n",
    "# This can significantly speed up training on modern GPUs\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"Variable dtype: {policy.variable_dtype}\")\n",
    "\n",
    "# Configure TensorFlow for optimal GPU usage\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA compilation\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "print(\"\\n✓ GPU optimization configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Model and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify we're in the correct directory\nimport os\nimport sys\n\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"Python path includes: {os.getcwd() in sys.path}\")\n\n# Check if required files exist\nrequired_files = ['model.py', 'utils.py']\nmissing_files = [f for f in required_files if not os.path.exists(f)]\nif missing_files:\n    print(f\"\\n✗ Missing files: {missing_files}\")\n    print(\"Make sure you ran the cell to clone the GitHub repository!\")\n    raise FileNotFoundError(f\"Required files not found: {missing_files}\")\n\n# Import from the cloned repository\nfrom model import get_model\nfrom utils import save_logs_models\n\nimport random\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import optimizers\nfrom time import gmtime, strftime\n\nprint(\"\\n✓ Model imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_N = 236\n",
    "frames = 100\n",
    "\n",
    "print(\"Loading HCP data from Kaggle dataset...\")\n",
    "# Load HCP data from Kaggle dataset\n",
    "with h5py.File(HCP_PATH, 'r') as f:\n",
    "    x_train, x_val, x_test = f['x_train'][()], f['x_val'][()], f['x_test'][()]\n",
    "    y_train, y_val, y_test = f['y_train'][()], f['y_val'][()], f['y_test'][()]\n",
    "\n",
    "# Add channel dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_val = np.expand_dims(x_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "\n",
    "# Convert to categorical\n",
    "num_classes = 100\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\ny_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n✓ Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "k = 5  # Number of nearest neighbors\n",
    "batch_size = 16  # Increased for GPU (original: 8)\n",
    "epochs = 100\n",
    "l2_reg = 1e-4\n",
    "dp = 0.5\n",
    "lr = 1e-5\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Dropout: {dp}\")\n",
    "print(f\"  L2 regularization: {l2_reg}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Learning rate: {lr}\")\n",
    "print(f\"  k (neighbors): {k}\")\n",
    "\n",
    "# Setup output directory\n",
    "file_name = f'kaggle_k_{k}_l2_{l2_reg}_dp_{dp}'\n",
    "print(f\"\\nFile name: {file_name}\")\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "tmp_name = f'tmp/tmp_{file_name}_{strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())}.hdf5'\n",
    "print(f\"Model checkpoint: {tmp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model with FC matrix from Kaggle dataset...\")\n",
    "\n",
    "# Build model using FC matrix from Kaggle dataset\n",
    "model = get_model(\n",
    "    graph_path=FC_PATH,  # Use Kaggle dataset path\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    dp=dp,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=None,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "lr_hist = []\n",
    "\n",
    "class Lr_record(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        tmp = K.get_value(model.optimizer.learning_rate)\n",
    "        lr_hist.append(tmp)\n",
    "        print(f'Learning rate: {tmp}')\n",
    "\n",
    "lr_record = Lr_record()\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    filepath=tmp_name,\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# TensorBoard callback for visualization\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"Monitor training progress in real-time!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train model\n",
    "model_history = model.fit(\n",
    "    x_train, y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[checkpointer, lr_record, reduce_lr, earlystop, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading best model for evaluation...\\n\")\n",
    "\n",
    "# Load best model using FC matrix from Kaggle dataset\n",
    "model_best = get_model(\n",
    "    graph_path=FC_PATH,  # Use Kaggle dataset path\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=tmp_name,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model_best.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_results = model_best.evaluate(x=x_val, y=y_val, batch_size=batch_size, verbose=1)\n",
    "print(f\"\\nValidation Results - Loss: {val_results[0]:.4f}, Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = model_best.evaluate(x=x_test, y=y_test, batch_size=batch_size, verbose=1)\n",
    "print(f\"\\nTest Results - Loss: {test_results[0]:.4f}, Accuracy: {test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save logs and models\n",
    "save_logs_models(\n",
    "    model,\n",
    "    model_history,\n",
    "    acc=val_results[1],\n",
    "    folder='tmp/',\n",
    "    lr_hist=lr_hist,\n",
    "    file_name=file_name,\n",
    "    loss_name='loss',\n",
    "    acc_name='accuracy',\n",
    "    tmp_name=tmp_name\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy and loss\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(model_history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(model_history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(model_history.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax2.plot(model_history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Accuracy: {max(model_history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "gpuType": "t4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}