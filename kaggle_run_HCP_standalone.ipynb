{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGCN fMRI Analysis on Kaggle (Standalone Version)\n",
    "## No Internet or GitHub Required\n",
    "\n",
    "This is a standalone version that doesn't require cloning from GitHub.\n",
    "\n",
    "## ‚öôÔ∏è Pre-configured Settings\n",
    "\n",
    "This notebook is pre-configured with:\n",
    "- ‚úÖ **GPU T4 Accelerator** - Automatically enabled for faster training\n",
    "\n",
    "## üìã Setup Instructions\n",
    "\n",
    "**Before running this notebook, upload these files to Kaggle:**\n",
    "\n",
    "1. **Upload Python files** (from GitHub repo):\n",
    "   - `model.py` - Download from: https://github.com/ismailukman/GCN_fMRI/blob/main/model.py\n",
    "   - `utils.py` - Download from: https://github.com/ismailukman/GCN_fMRI/blob/main/utils.py\n",
    "   - Upload them using \"File\" ‚Üí \"Upload\" in Kaggle\n",
    "\n",
    "2. **Add your dataset:**\n",
    "   - Click \"Add Data\" ‚Üí Search for your fMRI dataset and add it\n",
    "   - The notebook expects data at: `/kaggle/input/fmri-data/`\n",
    "   - Should contain: \n",
    "     - `HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5`\n",
    "     - `FC.npy`\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages of this version:**\n",
    "- ‚úÖ No Internet connection required\n",
    "- ‚úÖ No git clone errors\n",
    "- ‚úÖ Faster startup (no cloning delay)\n",
    "- ‚úÖ Works even if GitHub is down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import os\nimport sys\nimport shutil\n\nprint(\"=\"*70)\nprint(\"COPYING PYTHON FILES FROM DATASET TO WORKING DIRECTORY\")\nprint(\"=\"*70)\n\n# Define paths\ndataset_model = '/kaggle/input/fmri-data/model.py'\ndataset_utils = '/kaggle/input/fmri-data/utils.py'\nworking_model = '/kaggle/working/model.py'\nworking_utils = '/kaggle/working/utils.py'\n\n# Copy model.py\nif os.path.exists(dataset_model):\n    shutil.copy2(dataset_model, working_model)\n    print(f\"‚úì Copied: {dataset_model} ‚Üí {working_model}\")\n    size_kb = os.path.getsize(working_model) / 1024\n    print(f\"  Size: {size_kb:.2f} KB\")\nelse:\n    print(f\"‚úó NOT FOUND: {dataset_model}\")\n    print(f\"  Make sure your dataset contains model.py\")\n\nprint()\n\n# Copy utils.py\nif os.path.exists(dataset_utils):\n    shutil.copy2(dataset_utils, working_utils)\n    print(f\"‚úì Copied: {dataset_utils} ‚Üí {working_utils}\")\n    size_kb = os.path.getsize(working_utils) / 1024\n    print(f\"  Size: {size_kb:.2f} KB\")\nelse:\n    print(f\"‚úó NOT FOUND: {dataset_utils}\")\n    print(f\"  Make sure your dataset contains utils.py\")\n\nprint()\nprint(\"=\"*70)\n\n# Verify both files are now in working directory\nif os.path.exists(working_model) and os.path.exists(working_utils):\n    print(\"‚úÖ SUCCESS! Both files are ready in working directory\")\n    print()\n    print(\"Files in /kaggle/working/:\")\n    for file in ['model.py', 'utils.py']:\n        path = f'/kaggle/working/{file}'\n        if os.path.exists(path):\n            size_kb = os.path.getsize(path) / 1024\n            print(f\"  ‚úì {file}: {size_kb:.2f} KB\")\nelse:\n    print(\"‚ùå ERROR: Files could not be copied\")\n    print()\n    print(\"Please check:\")\n    print(\"1. Your dataset is added to this notebook (Click 'Add Data')\")\n    print(\"2. Your dataset contains model.py and utils.py\")\n    print(\"3. The dataset path is /kaggle/input/fmri-data/\")\n    raise FileNotFoundError(\"Could not copy Python files from dataset\")\n\n# Add working directory to Python path\nif '/kaggle/working' not in sys.path:\n    sys.path.insert(0, '/kaggle/working')\n    print()\n    print(\"‚úì Added /kaggle/working to Python path\")\n\nprint()\nprint(\"=\"*70)\nprint(\"READY TO IMPORT!\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if required Python files exist\n",
    "required_files = ['model.py', 'utils.py']\n",
    "missing_files = [f for f in required_files if not os.path.exists(f'/kaggle/working/{f}')]\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå MISSING FILES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nThe following files are missing: {', '.join(missing_files)}\")\n",
    "    print(\"\\nPlease upload them to this notebook:\")\n",
    "    print(\"\\n1. Download from GitHub:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"   - https://github.com/ismailukman/GCN_fMRI/blob/main/{file}\")\n",
    "        print(f\"     (Click 'Raw' button, then Ctrl+S to save)\")\n",
    "    print(\"\\n2. Upload to Kaggle:\")\n",
    "    print(\"   - Click 'File' ‚Üí 'Upload' in the Kaggle menu\")\n",
    "    print(\"   - Select the downloaded files\")\n",
    "    print(\"   - Wait for upload to complete\")\n",
    "    print(\"\\n3. Restart this cell\")\n",
    "    print(\"=\"*70)\n",
    "    raise FileNotFoundError(f\"Missing required files: {missing_files}\")\n",
    "else:\n",
    "    print(\"‚úì All required Python files found!\")\n",
    "    for file in required_files:\n",
    "        file_path = f'/kaggle/working/{file}'\n",
    "        size_kb = os.path.getsize(file_path) / 1024\n",
    "        print(f\"  ‚úì {file}: {size_kb:.2f} KB\")\n",
    "\n",
    "# Add working directory to Python path\n",
    "if '/kaggle/working' not in sys.path:\n",
    "    sys.path.insert(0, '/kaggle/working')\n",
    "    print(\"\\n‚úì Added /kaggle/working to Python path\")\n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU/TPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Check for GPU\n",
    "print(f\"\\nGPU Available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# List all available devices\n",
    "print(\"\\nAvailable devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Enable memory growth for GPU to avoid OOM errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\nMemory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Define Kaggle dataset path\n",
    "KAGGLE_DATA_PATH = '/kaggle/input/fmri-data/'\n",
    "\n",
    "# Check if dataset directory exists\n",
    "if os.path.exists(KAGGLE_DATA_PATH):\n",
    "    print(f\"‚úì Kaggle dataset found at: {KAGGLE_DATA_PATH}\")\n",
    "    print(\"\\nDataset contents:\")\n",
    "    for item in os.listdir(KAGGLE_DATA_PATH):\n",
    "        item_path = os.path.join(KAGGLE_DATA_PATH, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
    "            print(f\"  {item}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚úó Dataset not found at: {KAGGLE_DATA_PATH}\")\n",
    "    print(\"\\nPlease add the fMRI dataset to this notebook:\")\n",
    "    print(\"1. Click 'Add Data' in the right panel\")\n",
    "    print(\"2. Search for your fMRI dataset\")\n",
    "    print(\"3. Click 'Add' to attach it to this notebook\")\n",
    "    raise FileNotFoundError(f\"Dataset not found at {KAGGLE_DATA_PATH}\")\n",
    "\n",
    "# Define file paths\n",
    "HCP_PATH = os.path.join(KAGGLE_DATA_PATH, 'HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5')\n",
    "FC_PATH = os.path.join(KAGGLE_DATA_PATH, 'FC.npy')\n",
    "\n",
    "# Verify required files\n",
    "print(\"\\nVerifying required files:\")\n",
    "files_to_check = {'HCP_rfMRI_100s4s_236_MGTR_matlab_train_val_test.h5': HCP_PATH, 'FC.npy': FC_PATH}\n",
    "for name, path in files_to_check.items():\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  ‚úì {name}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {name}: NOT FOUND\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "\n",
    "# Inspect HCP data structure\n",
    "print(\"\\nHCP data structure:\")\n",
    "with h5py.File(HCP_PATH, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        print(f\"  {key}: {f[key].shape}\")\n",
    "\n",
    "print(\"\\n‚úì All required files verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure GPU/TPU Settings for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mixed precision for better GPU performance\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision training (float16 with float32 accumulators)\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"Variable dtype: {policy.variable_dtype}\")\n",
    "\n",
    "# Configure TensorFlow for optimal GPU usage\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA compilation\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "print(\"\\n‚úì GPU optimization configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Model and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the uploaded Python files\n",
    "from model import get_model\n",
    "from utils import save_logs_models\n",
    "\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from time import gmtime, strftime\n",
    "\n",
    "print(\"‚úì Model and utils imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_N = 236\n",
    "frames = 100\n",
    "\n",
    "print(\"Loading HCP data from Kaggle dataset...\")\n",
    "with h5py.File(HCP_PATH, 'r') as f:\n",
    "    x_train, x_val, x_test = f['x_train'][()], f['x_val'][()], f['x_test'][()]\n",
    "    y_train, y_val, y_test = f['y_train'][()], f['y_val'][()], f['y_test'][()]\n",
    "\n",
    "# Add channel dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_val = np.expand_dims(x_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "\n",
    "# Convert to categorical\n",
    "num_classes = 100\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\ny_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n‚úì Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "k = 5  # Number of nearest neighbors\n",
    "batch_size = 16  # Increased for GPU (original: 8)\n",
    "epochs = 100\n",
    "l2_reg = 1e-4\n",
    "dp = 0.5\n",
    "lr = 1e-5\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Dropout: {dp}\")\n",
    "print(f\"  L2 regularization: {l2_reg}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Learning rate: {lr}\")\n",
    "print(f\"  k (neighbors): {k}\")\n",
    "\n",
    "# Setup output directory\n",
    "file_name = f'kaggle_k_{k}_l2_{l2_reg}_dp_{dp}'\n",
    "print(f\"\\nFile name: {file_name}\")\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "tmp_name = f'tmp/tmp_{file_name}_{strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())}.hdf5'\n",
    "print(f\"Model checkpoint: {tmp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model with FC matrix from Kaggle dataset...\")\n",
    "\n",
    "model = get_model(\n",
    "    graph_path=FC_PATH,\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    dp=dp,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=None,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "lr_hist = []\n",
    "\n",
    "class Lr_record(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        tmp = K.get_value(model.optimizer.learning_rate)\n",
    "        lr_hist.append(tmp)\n",
    "        print(f'Learning rate: {tmp}')\n",
    "\n",
    "lr_record = Lr_record()\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    filepath=tmp_name,\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"Monitor training progress in real-time!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "model_history = model.fit(\n",
    "    x_train, y_train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[checkpointer, lr_record, reduce_lr, earlystop, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading best model for evaluation...\\n\")\n",
    "\n",
    "model_best = get_model(\n",
    "    graph_path=FC_PATH,\n",
    "    ROI_N=ROI_N,\n",
    "    frames=frames,\n",
    "    kernels=[8, 8, 8, 16, 32, 32],\n",
    "    k=k,\n",
    "    l2_reg=l2_reg,\n",
    "    num_classes=num_classes,\n",
    "    weight_path=tmp_name,\n",
    "    skip=[0, 0]\n",
    ")\n",
    "\n",
    "model_best.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=optimizers.legacy.Adam(learning_rate=lr),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_results = model_best.evaluate(x=x_val, y=y_val, batch_size=batch_size, verbose=1)\n",
    "print(f\"\\nValidation Results - Loss: {val_results[0]:.4f}, Accuracy: {val_results[1]:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = model_best.evaluate(x=x_test, y=y_test, batch_size=batch_size, verbose=1)\n",
    "print(f\"\\nTest Results - Loss: {test_results[0]:.4f}, Accuracy: {test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logs_models(\n",
    "    model,\n",
    "    model_history,\n",
    "    acc=val_results[1],\n",
    "    folder='tmp/',\n",
    "    lr_hist=lr_hist,\n",
    "    file_name=file_name,\n",
    "    loss_name='loss',\n",
    "    acc_name='accuracy',\n",
    "    tmp_name=tmp_name\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training complete! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(model_history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(model_history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(model_history.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax2.plot(model_history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Accuracy: {max(model_history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "gpuType": "t4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}